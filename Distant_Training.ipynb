{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distant_Training.ipynb",
      "provenance": [],
      "mount_file_id": "1qYwxjXwLVN0Oz9A0z2diSRjsgH7I3hPk",
      "authorship_tag": "ABX9TyPTQMdIVFKCnDqfDCv7eOu/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikarraju/NLP_Project/blob/main/Distant_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdIeBVBl7o9V"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ADL/ADL_1/distnt_training_1.6.csv\",encoding='ISO-8859-1')\n",
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0CVvzdI92It",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bdc6c58-126f-463f-f003-660c13d1c3d2"
      },
      "source": [
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "import gensim.downloader\n",
        "#model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Colab Notebooks/NLP/GoogleNews_Vectors_300.bin', binary=True)\n",
        "model = gensim.downloader.load('glove-twitter-50')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqE4C8LGA0dM"
      },
      "source": [
        "def get_train_sentence_vectors(sentences,model):\n",
        "  words_to_integer_dict = dict()\n",
        "  integer_to_word_dict = dict()\n",
        "  print(\"No. of sentences = \",len(sentences))\n",
        "  sentence_vectors = []\n",
        "  max_sentence_length = 0\n",
        "  word_count = 1\n",
        "  for sent in sentences:\n",
        "    vec = []\n",
        "    sent_length = 0\n",
        "    for word in sent:\n",
        "      sent_length += 1\n",
        "      if words_to_integer_dict.get(word) == None:\n",
        "        if word in model:\n",
        "          words_to_integer_dict[word] = word_count\n",
        "          vec.append(word_count)\n",
        "          word_count += 1\n",
        "        else:\n",
        "          vec.append(0)\n",
        "      else:\n",
        "        vec.append(words_to_integer_dict[word])\n",
        "    max_sentence_length = max(sent_length,max_sentence_length)\n",
        "    sentence_vectors.append(vec)\n",
        "  return sentence_vectors,word_count,max_sentence_length,words_to_integer_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh8br6TVA1fj"
      },
      "source": [
        "import numpy as np\n",
        "def get_initial_embeddings(model_ft,word_count,words_to_integer_dict):\n",
        "  count_in_model = 0\n",
        "  embedding_matrix = [[0]*50]*(word_count+2)  #one for not in model one for not in train data\n",
        "  for word in words_to_integer_dict:\n",
        "    if word in model_ft:\n",
        "      count_in_model += 1\n",
        "      embedding_matrix[words_to_integer_dict[word]] = model[word]\n",
        "  embedding_matrix = np.asarray(embedding_matrix,dtype=float)\n",
        "  print(\"No. of embeddings in model = \",count_in_model)\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1haR5W4Mp150"
      },
      "source": [
        "def pad_input_vectors(sentence_vectors,max_sentence_length):\n",
        "  for i in range(len(sentence_vectors)):\n",
        "    sentence_vectors[i] = sentence_vectors[i][0:max_sentence_length]\n",
        "    for j in range(max_sentence_length-len(sentence_vectors[i])):\n",
        "      sentence_vectors[i].append(0)\n",
        "  #print(sentence_vectors[0:2])\n",
        "  data_x = np.asarray(sentence_vectors)\n",
        "  return data_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjfqwoAAGCuI"
      },
      "source": [
        "# CNN model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Concatenate, Reshape, Input, Embedding, Dense, Softmax\n",
        "from keras.models import Sequential,Model\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_trained_CNN_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length):\n",
        "  initializer = tf.keras.initializers.HeNormal()\n",
        "  input = Input(shape=(max_sentence_length,))\n",
        "  e = Embedding(word_count+2, 50, weights = [embedding_matrix], input_length=max_sentence_length)(input)\n",
        "  e.trainable = False\n",
        "  reshaped_embedding = Reshape((1,max_sentence_length,50,1))(e)\n",
        "  filter_2 = Conv2D(50,(2,50),activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(reshaped_embedding)\n",
        "  filter_3 = Conv2D(50,(3,50),activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(reshaped_embedding) \n",
        "  filter_4 = Conv2D(50,(4,50),activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(reshaped_embedding)\n",
        "  reshaped_filter_2 = Reshape((max_sentence_length-1,50,1))(filter_2)\n",
        "  reshaped_filter_3 = Reshape((max_sentence_length-2,50,1))(filter_3)\n",
        "  reshaped_filter_4 = Reshape((max_sentence_length-3,50,1))(filter_4)\n",
        "  max_pooling_2 = MaxPooling2D(pool_size=(max_sentence_length-1,1))(reshaped_filter_2)\n",
        "  max_pooling_3 = MaxPooling2D(pool_size=(max_sentence_length-2,1))(reshaped_filter_3)\n",
        "  max_pooling_4 = MaxPooling2D(pool_size=(max_sentence_length-3,1))(reshaped_filter_4)\n",
        "  concatenated_output = Concatenate()([max_pooling_2,max_pooling_3,max_pooling_4])\n",
        "  concatenated_output_reshaped = Reshape((1,150))(concatenated_output)\n",
        "  dense1_output = Dense(50,activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(concatenated_output_reshaped)\n",
        "  dense2_output = Dense(3,activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(dense1_output)\n",
        "  softmax_probs = Softmax()(dense2_output)\n",
        "\n",
        "  model3 = Model(input,softmax_probs)\n",
        "  print(model3.summary())\n",
        "  print(\"compiling model\")\n",
        "  model3.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  print(\"fitting model\")\n",
        "  model3.fit(train_x,train_y,epochs=2)\n",
        "  e.trainable = True\n",
        "  model3.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model3.fit(train_x,train_y,epochs=2)\n",
        "  return model3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhZ3EoHbF1oV"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "tweets1 = tweets[0:100000]\n",
        "tweets1 += tweets[799999:899999]\n",
        "\n",
        "labels = [0]*100000\n",
        "labels += [2]*100000\n",
        "labels1 = np.asarray(labels1)\n",
        "\n",
        "for i in range(len(tweets1)):\n",
        "  tweets1[i] = tweets1[i].split(\" \")\n",
        "\n",
        "sentence_vectors,word_count,max_sentence_length,words_to_integer_dict = get_train_sentence_vectors(tweets1,model)\n",
        "\n",
        "data_x = pad_input_vectors(sentence_vectors,max_sentence_length)\n",
        "\n",
        "embedding_matrix = get_initial_embeddings(model,word_count,words_to_integer_dict)\n",
        "\n",
        "CNN_model = get_trained_CNN_model(data_x,labels1,embedding_matrix,word_count,max_sentence_length)\n",
        "\n",
        "embeddings = CNN_model.layers[1].get_weights()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL9-saXMblNH"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "words = []\n",
        "word_embeddings = []\n",
        "for each in words_to_integer_dict:\n",
        "  words.append(each)\n",
        "  word_embeddings.append(embeddings[words_to_integer_dict[each]])\n",
        "\n",
        "output_df = pd.DataFrame({'words':words,'embeds':word_embeddings},index=None)\n",
        "output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/NLP/Project/distant_training_embeddings.csv',index=False) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXp_hWPxjlLd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "def get_embeds_matrix(embeds_list):\n",
        "  embeds_matrix = []\n",
        "  for i in range(len(embeds_list)):\n",
        "    embeds_list[i] = str(embeds_list[i][1:len(embeds_list[i])-1])\n",
        "    array = embeds_list[i].split()\n",
        "    for j in range(0,len(array)):\n",
        "      array[j] = float(array[j])\n",
        "    embeds_matrix.append(array)\n",
        "  \n",
        "  return np.asarray(embeds_matrix)\n",
        "\n",
        "embeds_file = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Project/distant_training_embeddings.csv\")\n",
        "words_list = embeds_file['words'].tolist()\n",
        "embeds_list = embeds_file['embeds'].tolist()\n",
        "embeds_matrix = get_embeds_matrix(embeds_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L59Z9lMho4si"
      },
      "source": [
        "import math\n",
        "def cosine_similarity(vec_1,vec_2):\n",
        "  norm1 = 0\n",
        "  for i in range(len(vec_1)):\n",
        "    norm1 += vec_1[i]**2\n",
        "  norm2 = 0\n",
        "  for i in range(len(vec_2)):\n",
        "    norm2 += vec_2[i]**2\n",
        "  norm1 = math.sqrt(norm1)\n",
        "  norm2 = math.sqrt(norm2)\n",
        "  cosine = 0\n",
        "  for i in range(len(vec_1)):\n",
        "    cosine += vec_1[i]*vec_2[i]\n",
        "  return (cosine/(norm1*norm2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBxpc_6Xp_Ip"
      },
      "source": [
        "print(cosine_similarity(embeds_matrix[words_list.index('good')],embeds_matrix[words_list.index('bad')]))\n",
        "print(cosine_similarity(model.wv['good'],model.wv['bad']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyVIL_mZkUsT"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "X_embedded = TSNE(n_components=2).fit_transform(embeds_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJV5FfjZoluV"
      },
      "source": [
        "print(X_embedded[words_list.index('good')])\n",
        "print(X_embedded[words_list.index('bad')])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}