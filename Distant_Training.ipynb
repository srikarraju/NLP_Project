{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distant_Training.ipynb",
      "provenance": [],
      "mount_file_id": "1qYwxjXwLVN0Oz9A0z2diSRjsgH7I3hPk",
      "authorship_tag": "ABX9TyMWxdASL1getj2u1OfxTt6u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikarraju/NLP_Project/blob/main/Distant_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdIeBVBl7o9V",
        "outputId": "3ae65554-1dae-4d3e-f445-33154073831d"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ADL/ADL_1/distnt_training_1.6.csv\",encoding='ISO-8859-1')\n",
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['0', '1467810369', 'Mon Apr 06 22:19:45 PDT 2009', 'NO_QUERY',\n",
            "       '_TheSpecialOne_',\n",
            "       '@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08QVMTKX9Cyc",
        "outputId": "639ca973-7d5e-422d-8567-dc0c360de975"
      },
      "source": [
        "labels = df['0'].tolist()\n",
        "col_str = \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"\n",
        "tweets = df[col_str].tolist()\n",
        "print(labels[0:10])\n",
        "print(tweets[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', 'my whole body feels itchy and like its on fire ', \"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \", '@Kwesidei not the whole crew ', 'Need a hug ', \"@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\", \"@Tatiana_K nope they didn't have it \", '@twittera que me muera ? ', \"spring break in plain city... it's snowing \"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0CVvzdI92It",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bdc6c58-126f-463f-f003-660c13d1c3d2"
      },
      "source": [
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "import gensim.downloader\n",
        "#model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Colab Notebooks/NLP/GoogleNews_Vectors_300.bin', binary=True)\n",
        "model = gensim.downloader.load('glove-twitter-50')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqE4C8LGA0dM"
      },
      "source": [
        "def get_train_sentence_vectors(sentences,model):\n",
        "  words_to_integer_dict = dict()\n",
        "  integer_to_word_dict = dict()\n",
        "  print(\"No. of sentences = \",len(sentences))\n",
        "  sentence_vectors = []\n",
        "  max_sentence_length = 0\n",
        "  word_count = 1\n",
        "  for sent in sentences:\n",
        "    vec = []\n",
        "    sent_length = 0\n",
        "    for word in sent:\n",
        "      sent_length += 1\n",
        "      if words_to_integer_dict.get(word) == None:\n",
        "        if word in model:\n",
        "          words_to_integer_dict[word] = word_count\n",
        "          vec.append(word_count)\n",
        "          word_count += 1\n",
        "        else:\n",
        "          vec.append(0)\n",
        "      else:\n",
        "        vec.append(words_to_integer_dict[word])\n",
        "    max_sentence_length = max(sent_length,max_sentence_length)\n",
        "    sentence_vectors.append(vec)\n",
        "  return sentence_vectors,word_count,max_sentence_length,words_to_integer_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh8br6TVA1fj"
      },
      "source": [
        "import numpy as np\n",
        "def get_initial_embeddings(model_ft,word_count,words_to_integer_dict):\n",
        "  count_in_model = 0\n",
        "  embedding_matrix = [[0]*50]*(word_count+2)  #one for not in model one for not in train data\n",
        "  for word in words_to_integer_dict:\n",
        "    if word in model_ft:\n",
        "      count_in_model += 1\n",
        "      embedding_matrix[words_to_integer_dict[word]] = model[word]\n",
        "  embedding_matrix = np.asarray(embedding_matrix,dtype=float)\n",
        "  print(\"No. of embeddings in model = \",count_in_model)\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1haR5W4Mp150"
      },
      "source": [
        "def pad_input_vectors(sentence_vectors,max_sentence_length):\n",
        "  for i in range(len(sentence_vectors)):\n",
        "    sentence_vectors[i] = sentence_vectors[i][0:max_sentence_length]\n",
        "    for j in range(max_sentence_length-len(sentence_vectors[i])):\n",
        "      sentence_vectors[i].append(0)\n",
        "  #print(sentence_vectors[0:2])\n",
        "  data_x = np.asarray(sentence_vectors)\n",
        "  return data_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjfqwoAAGCuI"
      },
      "source": [
        "# CNN model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Concatenate, Reshape, Input, Embedding, Dense, Softmax\n",
        "from keras.models import Sequential,Model\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_trained_CNN_model(train_x,train_y,embedding_matrix,word_count,max_sentence_length):\n",
        "  initializer = tf.keras.initializers.HeNormal()\n",
        "  input = Input(shape=(max_sentence_length,))\n",
        "  e = Embedding(word_count+2, 50, weights = [embedding_matrix], input_length=max_sentence_length)(input)\n",
        "  e.trainable = False\n",
        "  reshaped_embedding = Reshape((1,max_sentence_length,50,1))(e)\n",
        "  filter_2 = Conv2D(50,(2,50),activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(reshaped_embedding)\n",
        "  filter_3 = Conv2D(50,(3,50),activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(reshaped_embedding) \n",
        "  filter_4 = Conv2D(50,(4,50),activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(reshaped_embedding)\n",
        "  reshaped_filter_2 = Reshape((max_sentence_length-1,50,1))(filter_2)\n",
        "  reshaped_filter_3 = Reshape((max_sentence_length-2,50,1))(filter_3)\n",
        "  reshaped_filter_4 = Reshape((max_sentence_length-3,50,1))(filter_4)\n",
        "  max_pooling_2 = MaxPooling2D(pool_size=(max_sentence_length-1,1))(reshaped_filter_2)\n",
        "  max_pooling_3 = MaxPooling2D(pool_size=(max_sentence_length-2,1))(reshaped_filter_3)\n",
        "  max_pooling_4 = MaxPooling2D(pool_size=(max_sentence_length-3,1))(reshaped_filter_4)\n",
        "  concatenated_output = Concatenate()([max_pooling_2,max_pooling_3,max_pooling_4])\n",
        "  concatenated_output_reshaped = Reshape((1,150))(concatenated_output)\n",
        "  dense1_output = Dense(50,activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(concatenated_output_reshaped)\n",
        "  dense2_output = Dense(3,activation='relu',kernel_initializer=initializer,bias_initializer=initializer,\n",
        "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),bias_regularizer=tf.keras.regularizers.l2(1e-4))(dense1_output)\n",
        "  softmax_probs = Softmax()(dense2_output)\n",
        "\n",
        "  model3 = Model(input,softmax_probs)\n",
        "  print(model3.summary())\n",
        "  print(\"compiling model\")\n",
        "  model3.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  print(\"fitting model\")\n",
        "  model3.fit(train_x,train_y,epochs=2)\n",
        "  e.trainable = True\n",
        "  model3.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "  model3.fit(train_x,train_y,epochs=2)\n",
        "  return model3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvruoB_kC_Fu",
        "outputId": "7d493b6c-55ac-48c5-c561-aca0d6fd9dcb"
      },
      "source": [
        "seen_1,seen_2 = 0,0\n",
        "for i in range(len(labels)):\n",
        "  if labels[i] == 4 and seen_1==0:\n",
        "    seen_1 = 1\n",
        "    print(i)\n",
        "    labels[i]=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "799999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCag-G5bEKHO"
      },
      "source": [
        "tweets1 = tweets[0:100000]\n",
        "tweets1 += tweets[799999:899999]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JccI6oMPG59r"
      },
      "source": [
        "import numpy as np\n",
        "labels1 = labels[0:100000]\n",
        "labels1 += [2]*100000\n",
        "labels1 = np.asarray(labels1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhZ3EoHbF1oV"
      },
      "source": [
        "for i in range(len(tweets1)):\n",
        "  tweets1[i] = tweets1[i].split(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHecftHrBCvZ",
        "outputId": "e94d4013-3d93-4b17-a7e1-4c7d0445f9ae"
      },
      "source": [
        "sentence_vectors,word_count,max_sentence_length,words_to_integer_dict = get_train_sentence_vectors(tweets1,model)\n",
        "\n",
        "data_x = pad_input_vectors(sentence_vectors,max_sentence_length)\n",
        "\n",
        "embedding_matrix = get_initial_embeddings(model,word_count,words_to_integer_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of sentences =  200000\n",
            "No. of embeddings in model =  38969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoGKsxLeFNDj",
        "outputId": "3e54b29c-dfad-4cff-8d93-ae715fab1e81"
      },
      "source": [
        "print(tweets1[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is', 'upset', 'that', 'he', \"can't\", 'update', 'his', 'Facebook', 'by', 'texting', 'it...', 'and', 'might', 'cry', 'as', 'a', 'result', '', 'School', 'today', 'also.', 'Blah!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbhZV8GzCJz9",
        "outputId": "74e6e11e-9459-4255-ba4c-4a5070d75e09"
      },
      "source": [
        "CNN_model = get_trained_CNN_model(data_x,labels1,embedding_matrix,word_count,max_sentence_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 110)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 110, 50)      1948600     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_15 (Reshape)            (None, 1, 110, 50, 1 0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 1, 109, 1, 50 5050        reshape_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 1, 108, 1, 50 7550        reshape_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 1, 107, 1, 50 10050       reshape_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_16 (Reshape)            (None, 109, 50, 1)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_17 (Reshape)            (None, 108, 50, 1)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_18 (Reshape)            (None, 107, 50, 1)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 1, 50, 1)     0           reshape_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 1, 50, 1)     0           reshape_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 1, 50, 1)     0           reshape_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 1, 50, 3)     0           max_pooling2d_9[0][0]            \n",
            "                                                                 max_pooling2d_10[0][0]           \n",
            "                                                                 max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "reshape_19 (Reshape)            (None, 1, 150)       0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1, 50)        7550        reshape_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1, 3)         153         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "softmax_3 (Softmax)             (None, 1, 3)         0           dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,978,953\n",
            "Trainable params: 1,978,953\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "compiling model\n",
            "fitting model\n",
            "Epoch 1/2\n",
            "6250/6250 [==============================] - 388s 62ms/step - loss: 0.6221 - accuracy: 0.7071\n",
            "Epoch 2/2\n",
            "6250/6250 [==============================] - 385s 62ms/step - loss: 0.4979 - accuracy: 0.7735\n",
            "Epoch 1/2\n",
            "6250/6250 [==============================] - 406s 65ms/step - loss: 0.4629 - accuracy: 0.7922\n",
            "Epoch 2/2\n",
            "6250/6250 [==============================] - 405s 65ms/step - loss: 0.4291 - accuracy: 0.8116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idsFCS8eSUjm"
      },
      "source": [
        "embeddings = CNN_model.layers[1].get_weights()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL9-saXMblNH"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "words = []\n",
        "word_embeddings = []\n",
        "for each in words_to_integer_dict:\n",
        "  words.append(each)\n",
        "  word_embeddings.append(embeddings[words_to_integer_dict[each]])\n",
        "\n",
        "output_df = pd.DataFrame({'words':words,'embeds':word_embeddings},index=None)\n",
        "output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/NLP/Project/distant_training_embeddings.csv',index=False) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCeYgwGEdErc",
        "outputId": "b02340bc-c21f-4b5b-b203-f7819a28a346"
      },
      "source": [
        "print(embeddings[words_to_integer_dict['good']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.9088729  -0.38718292 -0.18096112 -0.02151196  0.556913    0.9932972\n",
            "  1.5273864   0.31223163 -0.9377556   0.19941017 -0.96523255  0.48952287\n",
            " -4.695946   -0.28799155 -0.08227208  0.06594674  0.62876326 -0.11122999\n",
            " -1.1907783   0.4457765  -0.8488463  -0.17991054 -0.29931265  0.77638113\n",
            "  0.33496284  0.74343866  0.61681217 -0.02405708  1.0758072  -0.52666056\n",
            " -0.30289677  0.39240065 -0.0980799   0.47156036  0.60399926  0.7055437\n",
            "  0.19352466  0.04299197  0.01745365  1.5401787  -1.2847718  -0.31853494\n",
            "  0.800926    0.493639    0.3038349   0.67784417 -0.22689356 -0.28241208\n",
            " -0.0377995   0.21563646]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftulk8d_fRBS",
        "outputId": "0f34edbd-79d1-4d26-bfcf-f461f5aef5fa"
      },
      "source": [
        "print(model.wv['good'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.6608   -0.10159   0.026775 -0.088053  0.15578   0.87288   1.29\n",
            "  0.28934  -0.59205   0.26779  -0.76604   0.27955  -5.1483   -0.056899\n",
            " -0.050798 -0.083225  0.48048  -0.35409  -1.0566    0.065436 -0.46674\n",
            "  0.13847  -0.22022   0.61591   0.18462   0.77965   0.29022  -0.24679\n",
            "  0.95335  -0.35699  -0.24246   0.35939  -0.16369   0.30926   0.32784\n",
            "  0.66924  -0.028869  0.13981   0.12371   0.96181  -1.4018   -0.19285\n",
            "  0.79053   0.36647   0.32751   0.29666  -0.039173 -0.14523  -0.19663\n",
            "  0.026827]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJB1cM5Wdxzp",
        "outputId": "8cd6959d-447f-413a-e044-2f84e1463a35"
      },
      "source": [
        "print(embeddings[words_to_integer_dict['bad']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.6667318   0.17410867 -0.76818365 -0.14902386 -0.5615814   0.13316992\n",
            "  1.2899168   0.51425874  0.20434889  0.9541428   0.1850975  -0.46439135\n",
            " -4.665776    0.11483898  0.3595791  -0.09103412  0.23399332 -0.37052622\n",
            " -0.82646435 -0.69965285 -0.27986917  0.7830526   0.68211406  0.7676632\n",
            "  0.10351181  0.7002878  -0.33383143 -0.18528323 -0.84498984 -0.31981003\n",
            " -0.7581209  -0.34977284 -0.06020527 -0.36154577 -0.6455438   0.66562253\n",
            " -1.1050513  -0.2788534   0.07307023  0.23305412 -0.95882726  0.54246455\n",
            "  0.34790987  0.03235342 -0.5768586  -0.7598304   0.96125615  0.9636172\n",
            " -0.17310408  0.6728128 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXp_hWPxjlLd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "def get_embeds_matrix(embeds_list):\n",
        "  embeds_matrix = []\n",
        "  for i in range(len(embeds_list)):\n",
        "    embeds_list[i] = str(embeds_list[i][1:len(embeds_list[i])-1])\n",
        "    array = embeds_list[i].split()\n",
        "    for j in range(0,len(array)):\n",
        "      array[j] = float(array[j])\n",
        "    embeds_matrix.append(array)\n",
        "  \n",
        "  return np.asarray(embeds_matrix)\n",
        "\n",
        "embeds_file = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/Project/distant_training_embeddings.csv\")\n",
        "words_list = embeds_file['words'].tolist()\n",
        "embeds_list = embeds_file['embeds'].tolist()\n",
        "embeds_matrix = get_embeds_matrix(embeds_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L59Z9lMho4si"
      },
      "source": [
        "import math\n",
        "def cosine_similarity(vec_1,vec_2):\n",
        "  norm1 = 0\n",
        "  for i in range(len(vec_1)):\n",
        "    norm1 += vec_1[i]**2\n",
        "  norm2 = 0\n",
        "  for i in range(len(vec_2)):\n",
        "    norm2 += vec_2[i]**2\n",
        "  norm1 = math.sqrt(norm1)\n",
        "  norm2 = math.sqrt(norm2)\n",
        "  cosine = 0\n",
        "  for i in range(len(vec_1)):\n",
        "    cosine += vec_1[i]*vec_2[i]\n",
        "  return (cosine/(norm1*norm2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB4DWLPypcAu",
        "outputId": "5a8098ab-c784-4583-ada0-821cd0435b52"
      },
      "source": [
        "print(cosine_similarity(embeds_matrix[words_list.index('good')],embeds_matrix[words_list.index('amazing')]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7863735700087224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBxpc_6Xp_Ip",
        "outputId": "604fd2b2-77d3-4f74-8860-05e9ee54deac"
      },
      "source": [
        "print(cosine_similarity(model.wv['good'],model.wv['amazing']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8494970125985905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyVIL_mZkUsT"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "X_embedded = TSNE(n_components=2).fit_transform(embeds_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJV5FfjZoluV",
        "outputId": "e82070f8-3a0a-452b-a742-2f3ca436a476"
      },
      "source": [
        "print(X_embedded[words_list.index('good')])\n",
        "print(X_embedded[words_list.index('bad')])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[52.84215  33.566704]\n",
            "[59.97386  18.976067]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}